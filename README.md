Multimodal Suicide Risk Detection using DAIC-WOZ Dataset
This repository contains the implementation of a multimodal deep learning framework for suicide risk detection using the DAIC-WOZ dataset. The model leverages textual (RoBERTa) and audio (ComParE16) features, integrating them using a cross-modal attention mechanism for improved classification performance.
ğŸš€ Objective
To develop a system that classifies participants in the DAIC-WOZ dataset as at-risk or not-at-risk for suicide using multiple modalities: text and audio.
ğŸ“‚ Project Structure
multimodal_suicide_detection/
â”‚
â”œâ”€â”€ config/ # Configuration files
â”‚   â””â”€â”€ params.yaml # Hyperparameters and settings
â”‚
â”œâ”€â”€ data/ # Processed feature files and splits
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ text/ # RoBERTa embeddings (.pt files)
â”‚   â”‚   â””â”€â”€ audio/ # ComParE16 audio features (.csv or .pt)
â”‚   â””â”€â”€ labels.csv # Ground-truth PHQ8 scores or binary labels
â”‚
â”œâ”€â”€ src/ # Source code
â”‚   â”œâ”€â”€ models/ # Model definitions
â”‚   â”‚   â”œâ”€â”€ text_model.py # RoBERTa-based classifier
â”‚   â”‚   â”œâ”€â”€ audio_model.py # GRU-based classifier for audio features
â”‚   â”‚   â””â”€â”€ fusion_model.py # Cross-modal attention fusion model
â”‚   â”œâ”€â”€ preprocess/ # Preprocessing and feature extraction scripts
â”‚   â”‚   â”œâ”€â”€ extract_text_features.py
â”‚   â”‚   â”œâ”€â”€ extract_audio_features.py
â”‚   â”‚   â””â”€â”€ preprocess_labels.py
â”‚   â”œâ”€â”€ utils/ # Utility scripts (loaders, metrics, etc.)
â”‚   â”œâ”€â”€ train.py # Main training script for fusion model
â”‚   â””â”€â”€ evaluate.py # Evaluation script
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation

ğŸ§  Modalities Used

Text: Combined prompt-response utterances from transcripts were encoded using RoBERTa.
Audio: ComParE16 features extracted using openSMILE toolkit.
Fusion: Attention-based module for combining modalities, followed by a classification head.

ğŸ“Š Results



Modality
F1 Score



Text (RoBERTa)
0.76


Audio (ComParE16)
0.75


Fusion (Text + Audio)
0.79


âš™ï¸ Setup

Clone this repository:

git clone https://github.com/vinay-samineni/multimodal_suicide_detection.git
cd multimodal_suicide_detection


Install dependencies:

pip install -r requirements.txt


Download and preprocess the DAIC-WOZ dataset (requires access approval from provider; it takes around 7 days for reply).

Run feature extraction scripts:


python src/preprocess/extract_text_features.py
python src/preprocess/extract_audio_features.py
python src/preprocess/preprocess_labels.py


Train and evaluate the model:

python src/train.py
python src/evaluate.py

ğŸ“Œ Highlights

Handles utterance-level alignment between modalities using timestamps.
Uses SMOTE to handle class imbalance in training.
Final model achieves competitive F1 score on DAIC-WOZ benchmark.

ğŸ“œ Citation
If you use this code in your research, please cite:
@misc{vinay2025suicidedetection,
  title={Multimodal Suicide Detection using DAIC-WOZ Dataset},
  author={Vinay Samineni},
  year={2025},
  note={Undergraduate Research Project, RVR & JC College of Engineering}
}

ğŸ§‘â€ğŸ« Guide
Dr. M. Sridevi, NIT Trichy
ğŸ“¬ Contact
For questions or collaborations, feel free to contact [saminenivinay999@gmail.com].
